{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data analyses in Python\n",
    "\n",
    "Imagine you have a `.csv` file (could also be `.txt`, `.log`, `.dat`, `.sav`, etc.) with some data in it and want to analyze that data. Your `worklfow` to do so will most likely consist of different steps: `loading the data`, `\"cleaning\" the data`, `exploring the data`, `analyzing the data` and `visualization of the data/results`. In the following, we'll briefly go through all of these steps.\n",
    "\n",
    "As you might be aware, the first step is always to load necessary `packages` and/or `functions`. Most of the time, it is not clear what exactly is needed along the `worklfow`. Hence, starting with respective `packages/functions` we're sure about is a good idea. This is most likely based on the data you want to analyze. As we want to have a look at some data in a `.csv` file, `numpy` is a good starting point.\n",
    "However, to already provide you with the full list of `packages` we are going to explore, here you::\n",
    "\n",
    "- [numpy](https://numpy.org/)\n",
    "- [pandas](https://pandas.pydata.org/)\n",
    "- [scipy's stats module](https://docs.scipy.org/doc/scipy/reference/stats.html)\n",
    "- [statsmodels](http://www.statsmodels.org/stable/index.html)\n",
    "- [seaborn](seaborn.pydata.org)\n",
    "\n",
    "Ok, nuff said, let's go to work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `np.` + `tab` provides us with a nice overview of `numpy`'s functionality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case, we are not sure about how a certain function works or simply want to know more about it, we can use the `help` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(np.array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on our goal, the `genfromtxt` function looks useful, as we initially need to load the data from the `.csv` file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data_numpy = np.genfromtxt('data/brain_size.csv', delimiter=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that, we generated a variable called `my_data_numpy`. Now, let's check its `type`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(my_data_numpy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a `numpy.ndarray` and within it, the data is stored:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data_numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we saw in the `help` function above, `objects` and `data types` come with certain `functionality`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data_numpy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can, for example, check the `shape`, that is the `dimensionality`, of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data_numpy.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This returns not a `numpy.ndarray`, but a `tuple`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(my_data_numpy.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to `concatenate functionality`. E.g., we could `transpose` our `numpy.ndarray` and check its resulting `shape` within one command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data_numpy.transpose().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is it possible to `view only certain parts` of the data, i.e. the second row? Yes, using `slicing`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data_numpy[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output is a `numpy.ndarray` again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(my_data_numpy[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to be more specific, it is also possible to only view one value, i.e. the fourth value of the second row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data_numpy[1, 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the `data type` has changed to `numpy.float64`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(my_data_numpy[1, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, getting more than one value, i.e. multiple values of the second row, results in a `numpy.ndarray` again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data_numpy[1, 3:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at our data again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data_numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though it's a small dataset, there's already a lot going on: different `data types`, different `columns`, etc. and apparently not everything is `\"numpy\"` compatible. `Numpy` is a great tool and very powerful, building the foundation for a lot of `python libraries`, but in a lot of cases you might want to prefer `packages` that build upon `numpy` and are intended for specific purposes. A good example for that is the amazing `pandas` library that should be your first address for everything `data wrangling`. In particular, this refers to `tabular data`, that is `multiple observations` or `samples` described by a set of different `attributes` or `features`. The data can than be seen as a `2D table`, or `matrix`, with `columns` giving the different `attributes` of the data, and rows the `observations`. Let's try `pandas`, but at first, we have to import it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can check its functionality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`read_csv` looks helpful regarding loading the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data_pandas = pd.read_csv('data/brain_size.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do we have? A `pandas dataframe`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(my_data_pandas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before, our data was in `np.ndarray` format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(my_data_numpy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does our data look now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data_pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though we already have more information as in the `numpy array`, e.g., `headers`, `strings` and `indexes`, it still looks off. What's the problem? Well, we see that our data has a `;` as a `delimiter`, but we indicated `,` as delimiter when loading our data. Therefore, it is important to carefully check your data and beware of its specifics. Let's reload our data with the fitting `delimiter`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data_pandas = pd.read_csv('data/brain_size.csv', delimiter=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Investigating our `dataframe`, we see that it worked as expected this time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data_pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thinking about our `numpy.ndarray` version, we see a difference in the `shape` of the data, which is related to the `header`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data_pandas.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What can we do with our `dataframe`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data_pandas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example we can and should rename `columns` with uninformative names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data_pandas.rename(columns={'Unnamed: 0': 'sub-id'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks a bit more informative, doesn't it? Let's have a look at our columns again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data_pandas.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wait a minute, it's not `renamed`. Did we do something wrong? Let's check the respective functionality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(my_data_pandas.rename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the functionality more in depth, a `dataframe` with the new `column names` is returned, but the old one `not automatically changed`. Hence, we have to do it again, this overwriting the original `dataframe`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data_pandas=my_data_pandas.rename(columns={'Unnamed: 0': 'sub-id'})\n",
    "my_data_pandas.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas also allows the easy and fast `exploration` of our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data_pandas.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, not all `columns` are there. But why is that? We need to investigate the `columns` more closely, beginning with one that was included:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(my_data_pandas['sub-id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data in the `columns` is a `pandas series`, not a `dataframe` or `numpy.ndarray`, again with its own functionality. Nevertheless, it was included in our `numerical summary`. Let's check the first missing `column`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(my_data_pandas['Hair'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, that's not very informative on its own, as it's also a `pandas series`, but was not included. Maybe the `data type` is the problem? Luckily, the `pandas dataframe` object comes with a helpful functionality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data_pandas.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And a bit more closely using `indexing`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(my_data_pandas['Hair'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data in `my_data_pandas['Hair']` has the `type str` and as you might have already guessed: it's rather hard to compute `summary statistics` from a `str`. We could re-code it, but given there are only two values, this might not be very useful for our current aim:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data_pandas['Hair'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about the other `missing columns`, e.g., `height`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(my_data_pandas['Height'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `data type` is yet again `str`, but how many values do we have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data_pandas['Height'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hm, we can see that `height` contains `numerical values`. However, the `data type` is `str`. Here it can be useful to change the `data type`, using `pandas dataframe` object functionality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data_pandas['Height'].astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we're getting another `error`. This time, it's related to a `missing data point`, which needs to be addressed before the `conversion` is possible. We can simply use the `replace` functionality to `replace` the `missing data point` to `NaN`, which should as allow to do the `conversion`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data_pandas['Height'] = my_data_pandas['Height'].replace('.', np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data_pandas['Height'] = my_data_pandas['Height'].astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if the `column` is now included in the `summary`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data_pandas.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can do the same for the `Weight` column, `concatenating` all necessary functions in one line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data_pandas['Weight'] = my_data_pandas['Weight'].replace('.', np.nan).astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is `Weight` now included?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data_pandas.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also compute one statistical value for one column, for example the `mean` using `numpy`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(my_data_pandas['Weight'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But the same is also possible using inbuilt `pandas data frame` functionality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data_pandas['Weight'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do the same for the standard deviation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.std(my_data_pandas['Weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data_pandas['Weight'].std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see, the same `functionality` can lead to different `results`, potentially based on `different implementations`. Thus, always make sure to check every part of your code and re-run it to see if you get the same outputs. As you can see here, using a `jupyter notebook` for your analyses, this is comparably straightforward. Additionally, you can document each step of your workflow, from data loading, inspection, changes, etc. . While you should of course always use `version control` on your data, the format we've explored nicely allows to redo your analyses (excluding the `computational reproducibility` and `numerical instability` aspect). On top of that, you can document the executed steps so that your future self and everyone else knows what's going on. Enough chit-chat, now that we've loaded and inspected our data, as well as fixed some errors it's time to do some statistics. To show you a few nice `packages` that are out there, we will run different `analyses` with different `packages`. We will explore `pingouin`, `scipy`, `statsmodels` and `seaborn`.     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/raphaelvallat/pingouin/blob/master/docs/pictures/logo_pingouin.png?raw=true\" height=\"300\" width=\"700\"/>\n",
    "\n",
    "\n",
    "\n",
    "### _Pingouin is an open-source statistical package written in Python 3 and based mostly on Pandas and NumPy._\n",
    "\n",
    "\n",
    "- ANOVAs: one- and two-ways, repeated measures, mixed, ancova\n",
    "- Post-hocs tests and pairwise comparisons\n",
    "- Robust correlations\n",
    "- Partial correlation, repeated measures correlation and intraclass correlation\n",
    "- Linear/logistic regression and mediation analysis\n",
    "- Bayesian T-test and Pearson correlation\n",
    "- Tests for sphericity, normality and homoscedasticity\n",
    "- Effect sizes and power analysis\n",
    "- Parametric/bootstrapped confidence intervals around an effect size or a correlation coefficient\n",
    "- Circular statistics\n",
    "- Plotting: Bland-Altman plot, Q-Q plot, etc...\n",
    "\n",
    "**Pingouin is designed for users who want simple yet exhaustive statistical functions.**\n",
    "\n",
    "\n",
    "##### **material scavenged from [10 minutes to Pingouin](https://pingouin-stats.org/index.html) and [the pingouin docs](https://pingouin-stats.org/api.html)\n",
    "\n",
    "Let's import the `package`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pingouin as pg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlations\n",
    "\n",
    "\"In the broadest sense correlation is any statistical association, though in common usage it most often refers to how close two variables are to having a linear relationship with each other\" - [Wikipedia](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient)\n",
    "\n",
    "`Pingouin` supports a variety of [measures of correlation](https://pingouin-stats.org/generated/pingouin.corr.html#pingouin.corr). When talking about `correlation`, we commonly mean the `Pearson correlation coefficient`, also referred to as `Pearson's r`:\n",
    "\n",
    "<img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/93185aed3047ef42fa0f1b6e389a4e89a5654afa\"/>\n",
    "\n",
    "Computing `Pearson's r` using `pingouin` is as easy as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pearson_correlation = pg.corr(my_data_pandas['FSIQ'], my_data_pandas['VIQ'])\n",
    "display(pearson_correlation)\n",
    "cor_coeeficient = pearson_correlation['r']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output we got, is the `test summary`:\n",
    "\n",
    "- 'n' : Sample size (after NaN removal)\n",
    "- 'outliers' : number of outliers (only for 'shepherd' or 'skipped')\n",
    "- 'r' : Correlation coefficient\n",
    "- 'CI95' : [95% parametric confidence intervals](https://en.wikipedia.org/wiki/Confidence_interval)\n",
    "- 'r2' : [R-squared](https://en.wikipedia.org/wiki/Coefficient_of_determination)\n",
    "- 'adj_r2' : [Adjusted R-squared](https://en.wikipedia.org/wiki/Coefficient_of_determination#Adjusted_R2)\n",
    "- 'p-val' : one or two tailed p-value\n",
    "- 'BF10' : Bayes Factor of the alternative hypothesis (Pearson only)\n",
    "- 'power' : achieved power of the test (= 1 - type II error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we want to compute `pairwise correlations` between `columns` of a `dataframe`? With `pingouin` that's one line of code and we can even sort the results based on a `test statistic` of interest, e.g. `r2`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pg.pairwise_corr(my_data_pandas, columns=['FSIQ', 'VIQ', 'Weight']).sort_values(by=['r2'], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before we calculate:  `Testing statistical premises`\n",
    "\n",
    "Statistical procedures can be classfied into either [`parametric`](https://en.wikipedia.org/wiki/Parametric_statistics) or `non parametric` prcedures, which require different necessary preconditions to be met in order to show consistent/robust results. \n",
    "Generally people assume that their data follows a gaussian distribution, which allows for parametric tests to be run.\n",
    "Nevertheless it is essential to first test the distribution of your data to decide if the assumption of normally distributed data holds, if this is not the case we would have to switch to non parametric tests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Shapiro Wilk normality  test](https://pingouin-stats.org/generated/pingouin.normality.html#pingouin.normality)\n",
    "\n",
    "Standard procedure for testing `normal distribution` tests if the `distribution` of your data `deviates significantly` from a `normal distribution`.\n",
    "The function we're using returns the following information:\n",
    "\n",
    "- W  : Test statistic\n",
    "\n",
    "- p : float\n",
    "    P-value\n",
    "\n",
    "- normal : boolean\n",
    "    True if data comes from a normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pg.normality(my_data_pandas['Height'], alpha=.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Henze-Zirkler multivariate normality test](https://pingouin-stats.org/generated/pingouin.multivariate_normality.html#pingouin.multivariate_normality)\n",
    "\n",
    "The same procedure, but for [multivariate normal distributions](https://en.wikipedia.org/wiki/Multivariate_normal_distribution).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pg.multivariate_normality(my_data_pandas[['Height', 'Weight','VIQ']], alpha=.05)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Testing for homoscedasticity](https://pingouin-stats.org/generated/pingouin.homoscedasticity.html?highlight=homoscedasticity#pingouin.homoscedasticity)\n",
    "\n",
    "\n",
    "\"In statistics, a sequence or a vector of random variables is homoscedastic /ˌhoʊmoʊskəˈdæstɪk/ if all random variables in the sequence or vector have the same finite variance.\" - Wikipedia\n",
    "\n",
    "returns:\n",
    "\n",
    "equal_var : boolean True if data have equal variance.\n",
    "\n",
    "p : float P-value.\n",
    "\n",
    "Note: This function first tests if the data are normally distributed using the Shapiro-Wilk test. If yes, then the homogeneity of variances is measured using the Bartlett test. If the data are not normally distributed, the Levene test, which is less sensitive to departure from normality, is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pg.homoscedasticity(my_data_pandas[['VIQ', 'FSIQ']], alpha=.05)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parametric tests\n",
    "## Student's t-test: the simplest statistical test\n",
    "\n",
    "### 1-sample t-test: testing the value of a population mean\n",
    "\n",
    "tests if the population mean of data is likely to be equal to a given value (technically if observations are drawn from a Gaussian distributions of given population mean).\n",
    "\n",
    "\n",
    "`pingouin.ttest` returns the T_statistic, the p-value, the [degrees of freedom](https://en.wikipedia.org/wiki/Degrees_of_freedom_(statistics), the [Cohen d effect size](https://en.wikiversity.org/wiki/Cohen%27s_d), the achieved [power](https://en.wikipedia.org/wiki/Power_(statistics%29) of the test ( = 1 - type II error (beta) = [P(Reject H0|H1 is true)](https://deliveroo.engineering/2018/12/07/monte-carlo-power-analysis.html)), and the [Bayes Factor](https://en.wikipedia.org/wiki/Bayes_factor) of the alternative hypothesis\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pg.ttest(my_data_pandas['VIQ'],0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-sample t-test: testing for difference across populations\n",
    "\n",
    "We have seen above that the mean VIQ in the dark hair and light hair populations\n",
    "were different. To test if this is significant, we do a 2-sample t-test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "light_viq = my_data_pandas[my_data_pandas['Hair'] == 'light']['VIQ']\n",
    "dark_viq = my_data_pandas[my_data_pandas['Hair'] == 'dark']['VIQ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pg.ttest(light_viq, dark_viq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot achieved power of a paired T-test\n",
    "\n",
    "Plot the curve of achieved power given the effect size (Cohen d) and the sample size of a paired T-test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style='ticks', context='notebook', font_scale=1.2)\n",
    "\n",
    "d = 0.5  # Fixed effect size\n",
    "n = np.arange(5, 80, 5)  # Incrementing sample size\n",
    "\n",
    "# Compute the achieved power\n",
    "pwr = pg.power_ttest(d=d, n=n, contrast='paired', tail='two-sided')\n",
    "\n",
    "# Start the plot\n",
    "plt.plot(n, pwr, 'ko-.')\n",
    "plt.axhline(0.8, color='r', ls=':')\n",
    "plt.xlabel('Sample size')\n",
    "plt.ylabel('Power (1 - type II error)')\n",
    "plt.title('Achieved power of a paired T-test')\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non parametric tests:\n",
    "\n",
    "\n",
    "Unlike the parametric test these do not require the assumption of normal distributions.\n",
    "\n",
    "\"`Mann-Whitney U Test` (= Wilcoxon rank-sum test). It is the non-parametric version of the independent T-test.\n",
    "Mwu tests the hypothesis that data in x and y are samples from continuous distributions with equal medians. The test assumes that x and y are independent. This test corrects for ties and by default uses a continuity correction.\" - [mwu-function](https://pingouin-stats.org/generated/pingouin.mwu.html#pingouin.mwu)\n",
    "\n",
    "Test summary\n",
    "\n",
    "- 'W-val' : W-value\n",
    "- 'p-val' : p-value\n",
    "- 'RBC'   : matched pairs rank-biserial correlation (effect size)\n",
    "- 'CLES'  : common language effect size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pg.mwu(light_viq, dark_viq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"`Wilcoxon signed-rank test` is the non-parametric version of the paired T-test.\n",
    "\n",
    "The Wilcoxon signed-rank test tests the null hypothesis that two related paired samples come from the same distribution. A continuity correction is applied by default.\" - [wilcoxon - func](https://pingouin-stats.org/generated/pingouin.wilcoxon.html#pingouin.wilcoxon)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pg.wilcoxon(light_viq, dark_viq, tail='two-sided')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `scipy.stats` - Hypothesis testing: comparing two groups\n",
    "\n",
    "For simple [statistical tests](https://en.wikipedia.org/wiki/Statistical_hypothesis_testing), it is also possible to use the `scipy.stats` sub-module of [`scipy`](http://docs.scipy.org/doc/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-sample t-test: testing the value of a population mean\n",
    "\n",
    "`scipy.stats.ttest_1samp` tests if the population mean of data is likely to be equal to a given value (technically if observations are drawn from a Gaussian distributions of given population mean). It returns the [T statistic](https://en.wikipedia.org/wiki/Student%27s_t-test), and the [p-value](https://en.wikipedia.org/wiki/P-value) (see the function's help):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.ttest_1samp(my_data_pandas['VIQ'], 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a p-value of 10^-28 we can claim that the population mean for the IQ (VIQ measure) is not 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-sample t-test: testing for difference across populations\n",
    "\n",
    "We have seen above that the mean VIQ in the dark hair and light hair populations\n",
    "were different. To test if this is significant, we do a 2-sample t-test\n",
    "with `scipy.stats.ttest_ind`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "light_viq = my_data_pandas[my_data_pandas['Hair'] == 'light']['VIQ']\n",
    "dark_viq = my_data_pandas[my_data_pandas['Hair'] == 'dark']['VIQ']\n",
    "stats.ttest_ind(light_viq, dark_viq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paired tests: repeated measurements on the same indivuals\n",
    "\n",
    "PIQ, VIQ, and FSIQ give 3 measures of IQ. Let us test if FISQ and PIQ are significantly different. We can use a 2 sample test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.ttest_ind(my_data_pandas['FSIQ'], my_data_pandas['PIQ'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem with this approach is that it forgets that there are links\n",
    "between observations: FSIQ and PIQ are measured on the same individuals.\n",
    "\n",
    "Thus the variance due to inter-subject variability is confounding, and\n",
    "can be removed, using a \"paired test\", or [\"repeated measures test\"](https://en.wikipedia.org/wiki/Repeated_measures_design):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.ttest_rel(my_data_pandas['FSIQ'], my_data_pandas['PIQ'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is equivalent to a 1-sample test on the difference::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.ttest_1samp(my_data_pandas['FSIQ'] - my_data_pandas['PIQ'], 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "T-tests assume Gaussian errors. We can use a [Wilcoxon signed-rank test](https://en.wikipedia.org/wiki/Wilcoxon_signed-rank_test), that relaxes this assumption:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.wilcoxon(my_data_pandas['FSIQ'], my_data_pandas['PIQ'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** The corresponding test in the non paired case is the [Mann–Whitney U test](https://en.wikipedia.org/wiki/Mann%E2%80%93Whitney_U), `scipy.stats.mannwhitneyu`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "### Exercise 2\n",
    "\n",
    "* Test the difference between weights in people with dark and light hair.\n",
    "* Use non-parametric statistics to test the difference between VIQ in people with dark and light hair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "light_weight = my_data_pandas[my_data_pandas['Hair'] == 'light']['Weight']\n",
    "dark_weight = my_data_pandas[my_data_pandas['Hair'] == 'dark']['Weight']\n",
    "stats.ttest_ind(light_weight, dark_weight, nan_policy='omit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "stats.mannwhitneyu(light_viq, dark_viq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden"
   },
   "source": [
    "**Conclusion**: we find that the data does not support the hypothesis that people with dark and light hair have different VIQ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `statsmodels` - use \"formulas\" to specify statistical models in Python\n",
    "\n",
    "Use `statsmodels` to perform linear models, multiple factors or analysis of variance.\n",
    "\n",
    "\n",
    "## A simple linear regression\n",
    "\n",
    "Given two set of observations, `x` and `y`, we want to test the hypothesis that `y` is a linear function of `x`.\n",
    "\n",
    "In other terms:\n",
    "\n",
    "$y = x * coef + intercept + e$\n",
    "\n",
    "where $e$ is observation noise. We will use the [statsmodels](http://statsmodels.sourceforge.net) module to:\n",
    "\n",
    "1. Fit a linear model. We will use the simplest strategy, [ordinary least squares](https://en.wikipedia.org/wiki/Ordinary_least_squares) (OLS).\n",
    "2. Test that $coef$ is non zero.\n",
    "\n",
    "First, we generate simulated data according to the model. Then we specify an OLS model and fit it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.formula.api import ols\n",
    "model = ols(\"FSIQ ~ VIQ\", my_data_pandas).fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** For more about \"formulas\" for statistics in Python, see the [statsmodels documentation](http://statsmodels.sourceforge.net/stable/example_formulas.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can inspect the various statistics derived from the fit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Terminology\n",
    "\n",
    "Statsmodels uses a statistical terminology: the `y` variable in statsmodels is called *endogenous* while the `x` variable is called *exogenous*. This is discussed in more detail [here](http://statsmodels.sourceforge.net/devel/endog_exog.html). To simplify, `y` (endogenous) is the value you are trying to predict, while `x` (exogenous) represents the features you are using to make the prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "### Exercise 3\n",
    "\n",
    "Retrieve the estimated parameters from the model above.  \n",
    "**Hint**: use tab-completion to find the relevant attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "model.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical variables: comparing groups or multiple categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ols(\"VIQ ~ Hair + 1\", my_data_pandas).fit()\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tips on specifying model\n",
    " \n",
    "***Forcing categorical*** - the 'Hair' is automatically detected as a categorical variable, and thus each of its different values is treated as different entities.\n",
    "\n",
    "An integer column can be forced to be treated as categorical using:\n",
    "\n",
    "```python\n",
    "model = ols('VIQ ~ C(Hair)', my_data_pandas).fit()\n",
    "```\n",
    "\n",
    "***Intercept***: We can remove the intercept using `- 1` in the formula, or force the use of an intercept using `+ 1`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Link to t-tests between different FSIQ and PIQ\n",
    "\n",
    "To compare different types of IQ, we need to create a \"long-form\" table, listing IQs, where the type of IQ is indicated by a categorical variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_fisq = pd.DataFrame({'iq': my_data_pandas['FSIQ'], 'type': 'fsiq'})\n",
    "data_piq = pd.DataFrame({'iq': my_data_pandas['PIQ'], 'type': 'piq'})\n",
    "data_long = pd.concat((data_fisq, data_piq))\n",
    "print(data_long[::8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ols(\"iq ~ type\", data_long).fit()\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we retrieve the same values for t-test and corresponding p-values for the effect of the type of IQ than the previous t-test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.ttest_ind(my_data_pandas['FSIQ'], my_data_pandas['PIQ'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Regression: including multiple factors\n",
    "\n",
    "Consider a linear model explaining a variable `z` (the dependent\n",
    "variable) with 2 variables `x` and `y`:\n",
    "\n",
    "$z = x \\, c_1 + y \\, c_2 + i + e$\n",
    "\n",
    "Such a model can be seen in 3D as fitting a plane to a cloud of (`x`,\n",
    "`y`, `z`) points (see the following figure)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "x = np.linspace(-5, 5, 21)\n",
    "\n",
    "# We generate a 2D grid\n",
    "X, Y = np.meshgrid(x, x)\n",
    "\n",
    "# To get reproducable values, provide a seed value\n",
    "np.random.seed(1)\n",
    "\n",
    "# Z is the elevation of this 2D grid\n",
    "Z = -5 + 3*X - 0.5*Y + 8 * np.random.normal(size=X.shape)\n",
    "\n",
    "# Plot the data\n",
    "fig = plt.figure()\n",
    "ax = fig.gca(projection='3d')\n",
    "surf = ax.plot_trisurf(my_data_pandas['VIQ'].to_numpy(), my_data_pandas['PIQ'].to_numpy(), \n",
    "                       my_data_pandas['FSIQ'].to_numpy(), cmap=plt.cm.plasma)\n",
    "ax.view_init(20, -120)\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Y')\n",
    "ax.set_zlabel('Z')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ols('FSIQ ~ VIQ + PIQ', my_data_pandas).fit()\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-hoc hypothesis testing: analysis of variance (ANOVA)\n",
    "\n",
    "In the above iris example, we wish to test if the petal length is different between versicolor and virginica, after removing the effect of sepal width. This can be formulated as testing the difference between the coefficient associated to versicolor and virginica in the linear model estimated above (it is an Analysis of Variance, [ANOVA](https://en.wikipedia.org/wiki/Analysis_of_variance). For this, we write a **vector of 'contrast'** on the parameters estimated with an [F-test](https://en.wikipedia.org/wiki/F-test):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.f_test([0, 1, -1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is this difference significant?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "### Exercise 4\n",
    "\n",
    "Going back to the brain size + IQ data, test if the VIQ of people with dark and light hair are different after removing the effect of brain size, height, and weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/brain_size.csv', sep=';', na_values=\".\")\n",
    "model = ols(\"VIQ ~ Hair + Height + Weight + MRI_Count\", data).fit()\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Throwback to pingouin and pandas\n",
    "\n",
    "Remember `pingouin`? As briefly outlined, it can also compute `ANOVA`s and other types of models fairly easy. For example, let's compare `VIQ` between `light` and `dark` `hair`ed participants. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pg.anova(dv='VIQ', between='Hair', data=my_data_pandas,\n",
    "               detailed=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It gets even better, `pandas` actually support some `pingouin` functions directly as an in-built `dataframe method`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data_pandas.anova(dv='VIQ', between='Hair', detailed=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `seaborn` - use visualization for statistical exploration\n",
    "\n",
    "[Seaborn](http://stanford.edu/~mwaskom/software/seaborn/) combines simple statistical fits with plotting on `pandas dataframes`, `numpy arrays`, etc. ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pairplot: scatter matrices\n",
    "\n",
    "We can easily have an intuition on the interactions between continuous variables using `seaborn.pairplot` to display a scatter matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn\n",
    "seaborn.set()\n",
    "seaborn.pairplot(my_data_pandas, vars=['FSIQ', 'PIQ', 'VIQ'], kind='reg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Categorical variables can be plotted as the hue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seaborn.pairplot(my_data_pandas, vars=['FSIQ', 'VIQ', 'PIQ'], kind='reg', hue='Hair')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lmplot: plotting a univariate regression\n",
    "\n",
    "A regression capturing the relation between one variable and another, e.g. `FSIQ` and `VIQ`, can be plotted using `seaborn.lmplot`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seaborn.lmplot(y='FSIQ', x='VIQ', data=my_data_pandas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Robust regression\n",
    "Given that, in the above plot, there seems to be a couple of data points that are outside of the main cloud to the right, they might be outliers, not representative of the population, but driving the regression.\n",
    "\n",
    "To compute a regression that is less sensitive to outliers, one must use a [robust model](https://en.wikipedia.org/wiki/Robust_statistics). This is done in seaborn using ``robust=True`` in the plotting functions, or in `statsmodels` by replacing the use of the OLS by a \"Robust Linear Model\", `statsmodels.formula.api.rlm`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing for interactions\n",
    "\n",
    "Does `FSIQ` increase more with `PIQ` for people with dark hair than with light hair?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seaborn.lmplot(y='FSIQ', x='PIQ', hue='Hair', data=my_data_pandas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot above is made of two different fits. We need to formulate a single model that tests for a variance of slope across the population. This is done via an [\"interaction\"](http://statsmodels.sourceforge.net/devel/example_formulas.html#multiplicative-interactions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.formula.api import ols\n",
    "result = ols(formula='FSIQ ~ PIQ + Hair + PIQ * Hair', data=my_data_pandas).fit()\n",
    "print(result.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Take home messages\n",
    "\n",
    "* Hypothesis testing and p-value give you the **significance** of an effect / difference\n",
    "\n",
    "* **Formulas** (with categorical variables) enable you to express rich links in your data\n",
    "\n",
    "* **Visualizing** your data and simple model fits matters!\n",
    "\n",
    "* **Conditioning** (adding factors that can explain all or part of the variation) is an important modeling aspect that changes the interpretation."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
